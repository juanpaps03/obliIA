{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9f1c3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e4231adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4a591fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "311558bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(t, min_rate=0.1):\n",
    "    learning_step = 25\n",
    "    return max(min_rate, min(1, 1.0 - np.log10((t+1)/learning_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5c74be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(t, min_epsilon=0.01):\n",
    "    epsilon_step = 35\n",
    "    return max(min_epsilon, min(1, 1.0 - np.log10((t+1)/epsilon_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ac5a5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(state, Q, episode):\n",
    "    '''\n",
    "    Inicializar Q(s, a) arbitrariamente ∀s ∈ S, a ∈ A(s)\n",
    "    Repetir:\n",
    "        Inicializar s\n",
    "        done ← False\n",
    "        Repetir hasta done:\n",
    "            Con probabilidad ε hacer: (* estrategia ε-greedy *)\n",
    "                explore: a ← sample(A(s))\n",
    "                exploit: a ← arg m ́ax Q(s, ·)\n",
    "            s′, r , done ← step(a)\n",
    "            Q(s, a) ← Q(s, a) + α(r + γ max Q(s′, ·) − Q(s, a))\n",
    "            s ← s′\n",
    "    '''\n",
    "   \n",
    "    done = False\n",
    "    gamma = 0.999\n",
    "    while not done:\n",
    "        alpha = get_learning_rate(episode)\n",
    "        epsilon = get_epsilon(episode)\n",
    "        action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        current_Q_value = Q[state][action]\n",
    "        discrete_obs = get_state(obs)\n",
    "        Q[state][action] = current_Q_value + alpha*(reward + gamma*np.max(Q[discrete_obs][:]) - current_Q_value)\n",
    "        state = discrete_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "92a08789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72397952",
   "metadata": {},
   "source": [
    "# Discretizacion de variables de la observación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ebcbee",
   "metadata": {},
   "source": [
    "Como la posicion del carrito y su aceleracion no son tan relevantes como la posicion de la barra y su velocidad, se discretizan utilizando una menor cantidad de contenedores de mayor tamaño Por otro lado, tanto la aceleración angular como el ángulo del poste son las variables de mayor importancia, son los que van a variar con una mayor velocidad. Por lo tanto, se discretizan con una mayor cantidad de contenedores de menor tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "00564955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cart_pos_step:  0.96\n",
      "cart_acc_step:  10.0\n",
      "pole_angle_step:  0.03809090909090909\n",
      "angular_acc_step:  400.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "cart_position_bins, cart_pos_step = np.linspace(-2.4, 2.4, 6, retstep=True)\n",
    "cart_acc_bins, cart_acc_step = np.linspace(-10, 10, 3, retstep=True)\n",
    "pole_angle_bins, pole_angle_step = np.linspace(-.2095,.2095, 12, retstep=True)\n",
    "angular_acc_bins, angular_acc_step = np.linspace(-1000, 1000, 6, retstep=True)\n",
    "print(\"cart_pos_step: \", cart_pos_step)\n",
    "print(\"cart_acc_step: \", cart_acc_step)\n",
    "print(\"pole_angle_step: \", pole_angle_step)\n",
    "print(\"angular_acc_step: \", angular_acc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "345c50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    cart_pos = np.digitize(obs[0], cart_position_bins)\n",
    "    cart_acc = np.digitize(obs[1], cart_acc_bins)\n",
    "    pole_ang = np.digitize(obs[2], pole_angle_bins)\n",
    "    ang_acc = np.digitize(obs[3], angular_acc_bins)\n",
    "    state = tuple([cart_pos, cart_acc, pole_ang, ang_acc])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "17e0564c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3, 4)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = get_state(np.array([-1.4, 0.11, -0.100, 200]))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9ea75fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid cart position bins: 0 -  6\n",
      "Valid cart acceleration bins: 0 -  6\n",
      "Valid pole angle bins: 0 -  12\n",
      "Valid pole acceleration bins: 0 -  8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 7, 13, 9, 2)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_bins_count = cart_position_bins.size + 1\n",
    "acc_bins_count = cart_acc_bins.size + 1\n",
    "angle_bins_count = pole_angle_bins.size + 1\n",
    "angular_acc_bins_count = angular_acc_bins.size + 1\n",
    "print(\"Valid cart position bins: 0 - \", position_bins_count - 1)\n",
    "print(\"Valid cart acceleration bins: 0 - \", acc_bins_count - 1)\n",
    "print(\"Valid pole angle bins: 0 - \", angle_bins_count - 1)\n",
    "print(\"Valid pole acceleration bins: 0 - \", angular_acc_bins_count - 1)\n",
    "Q = np.random.random((position_bins_count,acc_bins_count,angle_bins_count,angular_acc_bins_count,2))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5de5d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "episode:  100\n",
      "episode:  200\n",
      "episode:  300\n",
      "episode:  400\n",
      "episode:  500\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "done = False\n",
    "max_episodes = 550\n",
    "for i in range(max_episodes):\n",
    "    if i % 100 == 0:\n",
    "        print('episode: ', i)\n",
    "    obs = env.reset()\n",
    "    q_learning(get_state(obs), Q, i)\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6eeb4e6",
   "metadata": {},
   "source": [
    "# Ejecución con la policy óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "df84dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing optimal policy\n",
      "Average reward:  198.36\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "max_reward = 475\n",
    "env = gym.make('CartPole-v1', new_step_api=True)\n",
    "tries = 100\n",
    "rewards = np.zeros(tries)\n",
    "print(\"Playing optimal policy\")\n",
    "for i in range(tries):\n",
    "    episode_reward= 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done and episode_reward < max_reward:\n",
    "        state = get_state(obs)\n",
    "        action = optimal_policy(state, Q)\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        #print('->', reward, obs, done)\n",
    "    rewards[i] = episode_reward\n",
    "env.close()\n",
    "print(\"Average reward: \", np.mean(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "287bffd5c227edae0a833da74c9e64d677e3d2a7622abfe7074e89f36b406f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
